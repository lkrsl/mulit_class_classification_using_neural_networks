{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Zemczcjvoo"
      },
      "source": [
        "\n",
        "\n",
        "# Multi-class Classification for Predicting Stock Price Movement of Amazon using Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uQRh-HN82lNg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import yfinance as yf\n",
        "import pandas as pd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oxwMkt9bhAeG"
      },
      "source": [
        "### Load Dataset\n",
        "\n",
        "We read the data and map the textual categories (upwards, downwards and sideways) to numerical class labels (0, 1 or 2) and split of the dataset into training (80%) and test (20%) datasets. The input features are normalised to have zero mean and unit standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fm/3hy0z8052093y_1_c5sjdnc00000gn/T/ipykernel_26365/167139457.py:2: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download('AMZN', start='2010-01-01', end='2023-12-31')\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw data shape: (3522, 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ticker</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMZN</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>6.6950</td>\n",
              "      <td>6.8305</td>\n",
              "      <td>6.6570</td>\n",
              "      <td>6.8125</td>\n",
              "      <td>151998000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>6.7345</td>\n",
              "      <td>6.7740</td>\n",
              "      <td>6.5905</td>\n",
              "      <td>6.6715</td>\n",
              "      <td>177038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>6.6125</td>\n",
              "      <td>6.7365</td>\n",
              "      <td>6.5825</td>\n",
              "      <td>6.7300</td>\n",
              "      <td>143576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>6.5000</td>\n",
              "      <td>6.6160</td>\n",
              "      <td>6.4400</td>\n",
              "      <td>6.6005</td>\n",
              "      <td>220604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>6.6760</td>\n",
              "      <td>6.6840</td>\n",
              "      <td>6.4515</td>\n",
              "      <td>6.5280</td>\n",
              "      <td>196610000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Price        Close    High     Low    Open     Volume\n",
              "Ticker        AMZN    AMZN    AMZN    AMZN       AMZN\n",
              "Date                                                 \n",
              "2010-01-04  6.6950  6.8305  6.6570  6.8125  151998000\n",
              "2010-01-05  6.7345  6.7740  6.5905  6.6715  177038000\n",
              "2010-01-06  6.6125  6.7365  6.5825  6.7300  143576000\n",
              "2010-01-07  6.5000  6.6160  6.4400  6.6005  220604000\n",
              "2010-01-08  6.6760  6.6840  6.4515  6.5280  196610000"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download historical Amazon stock data\n",
        "data = yf.download('AMZN', start='2010-01-01', end='2023-12-31')\n",
        "print(\"Raw data shape:\", data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset shape: (3493, 5)\n",
            "\n",
            "Class distribution:\n",
            "y\n",
            "sideways     1709\n",
            "upwards       985\n",
            "downwards     799\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset saved as 'amazon_stock_price.csv'\n",
            "\n",
            "Sample of the saved data:\n",
            "       x1       x2       x3       x4         y\n",
            "                                              \n",
            "-2.107274 2.765282 0.707773 0.951663 downwards\n",
            "-0.649186 1.349620 0.706223 0.954718   upwards\n",
            " 1.933702 3.185424 0.771826 0.958190  sideways\n",
            "-0.330765 1.772543 0.560496 0.963326  sideways\n",
            " 0.545277 2.377100 0.542223 0.966991  sideways\n",
            "-0.652484 2.321839 0.567164 0.971651   upwards\n",
            " 1.492036 2.246519 0.595190 0.975879 downwards\n",
            " 0.025381 2.107130 0.766930 0.979040  sideways\n",
            " 0.441127 2.061419 0.466541 0.980539   upwards\n",
            " 4.919971 6.006734 1.073920 0.985358  sideways\n",
            "Final dataset shape: (3493, 5)\n",
            "\n",
            "Class distribution:\n",
            "y\n",
            "sideways     1709\n",
            "upwards       985\n",
            "downwards     799\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset saved as 'amazon_stock_price.csv'\n",
            "\n",
            "Sample of the saved data:\n",
            "       x1       x2       x3       x4         y\n",
            "                                              \n",
            "-2.107274 2.765282 0.707773 0.951663 downwards\n",
            "-0.649186 1.349620 0.706223 0.954718   upwards\n",
            " 1.933702 3.185424 0.771826 0.958190  sideways\n",
            "-0.330765 1.772543 0.560496 0.963326  sideways\n",
            " 0.545277 2.377100 0.542223 0.966991  sideways\n",
            "-0.652484 2.321839 0.567164 0.971651   upwards\n",
            " 1.492036 2.246519 0.595190 0.975879 downwards\n",
            " 0.025381 2.107130 0.766930 0.979040  sideways\n",
            " 0.441127 2.061419 0.466541 0.980539   upwards\n",
            " 4.919971 6.006734 1.073920 0.985358  sideways\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering\n",
        "df = data.copy()\n",
        "\n",
        "# 1. Daily Return (x1): (Close - Open) / Open\n",
        "df['x1'] = (df['Close'] - df['Open']) / df['Open'] * 100  # as percentage\n",
        "\n",
        "# 2. Intraday Volatility (x2): (High - Low) / Open\n",
        "df['x2'] = (df['High'] - df['Low']) / df['Open'] * 100  # as percentage\n",
        "\n",
        "# 3. Volume Signal (x3): Volume / 30-day average volume\n",
        "df['x3'] = df['Volume'] / df['Volume'].rolling(window=30).mean()\n",
        "\n",
        "# 4. Price Trend (x4): 10-day MA / 30-day MA (momentum indicator)\n",
        "df['ma_10'] = df['Close'].rolling(window=10).mean()\n",
        "df['ma_30'] = df['Close'].rolling(window=30).mean()\n",
        "df['x4'] = df['ma_10'] / df['ma_30']\n",
        "\n",
        "# Create target variable (next day's movement)\n",
        "threshold = 1.0  # 1% threshold for meaningful movement\n",
        "df['next_day_return'] = df['Close'].pct_change().shift(-1) * 100\n",
        "\n",
        "df['y'] = 'sideways'  # default\n",
        "df.loc[df['next_day_return'] > threshold, 'y'] = 'upwards'\n",
        "df.loc[df['next_day_return'] < -threshold, 'y'] = 'downwards'\n",
        "\n",
        "# Keep only the columns we need and drop NaN values\n",
        "final_df = df[['x1', 'x2', 'x3', 'x4', 'y']].copy()\n",
        "final_df = final_df.dropna()\n",
        "\n",
        "print(\"Final dataset shape:\", final_df.shape)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(final_df['y'].value_counts())\n",
        "\n",
        "# Save to CSV in the exact format you need\n",
        "final_df.to_csv('amazon_stock_price.csv', index=False, header=False)\n",
        "print(\"\\nDataset saved as 'amazon_stock_price.csv'\")\n",
        "\n",
        "# Show sample of the final data\n",
        "print(\"\\nSample of the saved data:\")\n",
        "print(final_df.head(10).to_string(index=False))# Feature Engineering\n",
        "df = data.copy()\n",
        "\n",
        "# 1. Daily Return (x1): (Close - Open) / Open\n",
        "df['x1'] = (df['Close'] - df['Open']) / df['Open'] * 100  # as percentage\n",
        "\n",
        "# 2. Intraday Volatility (x2): (High - Low) / Open\n",
        "df['x2'] = (df['High'] - df['Low']) / df['Open'] * 100  # as percentage\n",
        "\n",
        "# 3. Volume Signal (x3): Volume / 30-day average volume\n",
        "df['x3'] = df['Volume'] / df['Volume'].rolling(window=30).mean()\n",
        "\n",
        "# 4. Price Trend (x4): 10-day MA / 30-day MA (momentum indicator)\n",
        "df['ma_10'] = df['Close'].rolling(window=10).mean()\n",
        "df['ma_30'] = df['Close'].rolling(window=30).mean()\n",
        "df['x4'] = df['ma_10'] / df['ma_30']\n",
        "\n",
        "# Create target variable (next day's movement)\n",
        "threshold = 1.0  # 1% threshold for meaningful movement\n",
        "df['next_day_return'] = df['Close'].pct_change().shift(-1) * 100\n",
        "\n",
        "df['y'] = 'sideways'  # default\n",
        "df.loc[df['next_day_return'] > threshold, 'y'] = 'upwards'\n",
        "df.loc[df['next_day_return'] < -threshold, 'y'] = 'downwards'\n",
        "\n",
        "# Keep only the columns we need and drop NaN values\n",
        "final_df = df[['x1', 'x2', 'x3', 'x4', 'y']].copy()\n",
        "final_df = final_df.dropna()\n",
        "\n",
        "print(\"Final dataset shape:\", final_df.shape)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(final_df['y'].value_counts())\n",
        "\n",
        "# Save to CSV in the exact format you need\n",
        "final_df.to_csv('amazon_stock_price.csv', index=False, header=False)\n",
        "print(\"\\nDataset saved as 'amazon_stock_price.csv'\")\n",
        "\n",
        "# Show sample of the final data\n",
        "print(\"\\nSample of the saved data:\")\n",
        "print(final_df.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uIwEs1HZH1JQ"
      },
      "outputs": [],
      "source": [
        "# Loading dataset and preparing data\n",
        "df = pd.read_csv('./amazon_stock_price.csv', index_col=None, header=None) # change the path to your own path\n",
        "df.columns = ['x1', 'x2', 'x3', 'x4', 'y']\n",
        "\n",
        "d = {'upwards': 1,\n",
        "     'downwards': 2,\n",
        "     'sideways': 0,\n",
        "}\n",
        "\n",
        "df['y'] = df['y'].map(d)\n",
        "\n",
        "# Assign features and target\n",
        "\n",
        "X = torch.tensor(df[['x1', 'x2', 'x3', 'x4']].values, dtype=torch.float)\n",
        "y = torch.tensor(df['y'].values, dtype=torch.int)\n",
        "\n",
        "# Shuffling & train/test split\n",
        "\n",
        "torch.manual_seed(123)\n",
        "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
        "\n",
        "X, y = X[shuffle_idx], y[shuffle_idx]\n",
        "\n",
        "percent80 = int(shuffle_idx.size(0)*0.8)\n",
        "\n",
        "X_train, X_test = X[shuffle_idx[:percent80]], X[shuffle_idx[percent80:]]\n",
        "y_train, y_test = y[shuffle_idx[:percent80]], y[shuffle_idx[percent80:]]\n",
        "\n",
        "# Normalize (mean zero, unit variance)\n",
        "\n",
        "mu, sigma = X_train.mean(dim=0), X_train.std(dim=0)\n",
        "X_train = (X_train - mu) / sigma\n",
        "X_test = (X_test - mu) / sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1)  Logistic Regression for Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: epoch: [10/100], loss: 0.6245\n",
            "Progress: epoch: [20/100], loss: 0.6027\n",
            "Progress: epoch: [30/100], loss: 0.5985\n",
            "Progress: epoch: [40/100], loss: 0.5963\n",
            "Progress: epoch: [50/100], loss: 0.5948\n",
            "Progress: epoch: [60/100], loss: 0.5939\n",
            "Progress: epoch: [70/100], loss: 0.5933\n",
            "Progress: epoch: [80/100], loss: 0.5927\n",
            "Progress: epoch: [90/100], loss: 0.5923\n",
            "Progress: epoch: [100/100], loss: 0.5919\n",
            "Accuracy of model 1: 50.21%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123) # for reproducibility \n",
        "\n",
        "class FeedforwardNeuralNetwork(nn.Module): # define class\n",
        "    def __init__(self, num_features, hidden_size, num_classes): # initialze neural network\n",
        "        super(FeedforwardNeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, hidden_size) # create first layer to transform input data using weights and biases\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes) # create second layer to produce final predictions\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # using the function from above - \"rectified linear unit\" transforming the input using the weights and biases of the first layer\n",
        "        x = torch.sigmoid(self.fc2(x)) # logistic sigmoid function squeezes the input values to a range between 0 and 1\n",
        "        return x\n",
        "\n",
        "# Initializing the model   \n",
        "num_features = 4 # give the task:'x1', 'x2', 'x3', 'x4'\n",
        "hidden_size = 10 # can be any number (h)\n",
        "num_classes = 3  # given the task:'upwards': 1,'downwards': 2,'sideways': 0\n",
        "model = FeedforwardNeuralNetwork(num_features, hidden_size, num_classes)\n",
        "\n",
        "# Initializing training parameters\n",
        "num_epochs = 100 # can be any number (try to avoid over-/underfitting by choosing an appropriate number)\n",
        "learning_rate = 1e-2 # can be any number (we choose this similar to the lecture)\n",
        "\n",
        "criterion = nn.BCELoss() # binary cross entropy loss (BCE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # updates weights and biases during training in the first layer (fc1)\n",
        "\n",
        "y_onehot = torch.zeros(y_train.size(0), num_classes) # initalizing a tensor with zeros (number of rows defined by y_train.size(0) and number of column by num_classes)\n",
        "y_train_long = y_train.long().unsqueeze(1) # convert to long datatype and add an extra dimension of 1 to store the one_hot encoded labels\n",
        "y_onehot.scatter_(1, y_train_long, 1) # fill one_hot encoded vector\n",
        "\n",
        "# Training using binary cross entropy loss for each output node\n",
        "for epoch in range(num_epochs): # iterating num_epochs\n",
        "    outputs = model(X_train) # output predictions for each epoch\n",
        "    loss = criterion(outputs, y_onehot) # loss is calculated using BCE\n",
        "    optimizer.zero_grad() # gradients are cleared from previous period\n",
        "    loss.backward() # gradients are computed\n",
        "    optimizer.step() # update\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Progress: \" f'epoch: [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}') #print output\n",
        "    \n",
        "# Testing\n",
        "with torch.no_grad(): # disabling gradient computation\n",
        "    outputs = model(X_test) # output predictions for test samples\n",
        "    _, predicted = torch.max(outputs.data, 1) # return indices of max values along dimension 1\n",
        "    correct_predicted = (predicted == y_test).sum().item() # compute number of correct predictions\n",
        "    accuracy_ffnn = 100 * correct_predicted / len(y_test) # percentage of correct predictions divided by total number of test samples\n",
        "    print(f'Accuracy of model 1: {accuracy_ffnn:.2f}%') # print accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2)  Softmax Regression with custom implementation of Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: epoch [10/100], loss: 1.0899\n",
            "Progress: epoch [20/100], loss: 1.0568\n",
            "Progress: epoch [30/100], loss: 1.0395\n",
            "Progress: epoch [40/100], loss: 1.0308\n",
            "Progress: epoch [50/100], loss: 1.0266\n",
            "Progress: epoch [60/100], loss: 1.0248\n",
            "Progress: epoch [70/100], loss: 1.0242\n",
            "Progress: epoch [80/100], loss: 1.0241\n",
            "Progress: epoch [90/100], loss: 1.0240\n",
            "Progress: epoch [100/100], loss: 1.0240\n",
            "Accuracy of model 2: 50.50%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123) # for reproducibility \n",
        "\n",
        "class SoftmaxRegression(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(SoftmaxRegression, self).__init__()\n",
        "        self.fc = nn.Linear(num_features, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encoding(labels, num_classes):\n",
        "    labels_long = labels.long().unsqueeze(1)\n",
        "    return torch.zeros(len(labels), num_classes).scatter_(1, labels_long, 1)\n",
        "\n",
        "# Softmax function\n",
        "def softmax(logits):\n",
        "    exp_logits = torch.exp(logits)\n",
        "    sum_exp_logits = exp_logits.sum(dim=1, keepdim=True)\n",
        "    probabilities = exp_logits / sum_exp_logits\n",
        "    return probabilities # all continous probabilities add up to 1\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(probabilities, one_hot_labels):\n",
        "    log_probs = torch.log(probabilities)\n",
        "    loss = -torch.sum(log_probs * one_hot_labels) / probabilities.shape[0]\n",
        "    return loss\n",
        "\n",
        "# Initializing the model\n",
        "num_features = 4 \n",
        "num_classes = 3\n",
        "model = SoftmaxRegression(num_features, num_classes)\n",
        "\n",
        "# Initializing training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    logits = model(X_train)\n",
        "    probabilities = softmax(logits)\n",
        "    one_hot_labels = one_hot_encoding(y_train, num_classes)\n",
        "    loss = cross_entropy_loss(probabilities, one_hot_labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Progress: \" f'epoch [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test)\n",
        "    probabilities = softmax(logits)\n",
        "    _, predicted = torch.max(probabilities, 1)\n",
        "    correct = (predicted == y_test).sum().item()\n",
        "    accuracy_sm_cel = 100 * correct / len(y_test)\n",
        "    print(f'Accuracy of model 2: {accuracy_sm_cel:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3)  Softmax Regression with torch.nn.functional.nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: epoch [10/100], loss: 1.0899\n",
            "Progress: epoch [20/100], loss: 1.0568\n",
            "Progress: epoch [30/100], loss: 1.0395\n",
            "Progress: epoch [40/100], loss: 1.0308\n",
            "Progress: epoch [50/100], loss: 1.0266\n",
            "Progress: epoch [60/100], loss: 1.0248\n",
            "Progress: epoch [70/100], loss: 1.0242\n",
            "Progress: epoch [80/100], loss: 1.0241\n",
            "Progress: epoch [90/100], loss: 1.0240\n",
            "Progress: epoch [100/100], loss: 1.0240\n",
            "Accuracy of model 3: 50.50%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)  # for reproducibility\n",
        "\n",
        "class SoftmaxRegression(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(SoftmaxRegression, self).__init__()\n",
        "        self.fc = nn.Linear(num_features, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x)\n",
        "        return F.log_softmax(logits, dim=1)  # log softmax \n",
        "\n",
        "# Initializing model\n",
        "num_features = 4\n",
        "num_classes = 3\n",
        "model = SoftmaxRegression(num_features, num_classes)\n",
        "\n",
        "# Initializing training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    log_probabilities = model(X_train) \n",
        "    loss = F.nll_loss(log_probabilities, y_train.long()) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Progress: \" f'epoch [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "with torch.no_grad():\n",
        "    log_probabilities = model(X_test) \n",
        "    _, predicted = torch.max(log_probabilities, 1)  \n",
        "    correct = (predicted == y_test).sum().item()\n",
        "    accuracy_sm_tn = 100 * correct / len(y_test)\n",
        "    print(f'Accuracy of model 3: {accuracy_sm_tn:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4)  Softmax Regression with torch.nn.functional.cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: epoch [10/100], loss: 1.0899\n",
            "Progress: epoch [20/100], loss: 1.0568\n",
            "Progress: epoch [30/100], loss: 1.0395\n",
            "Progress: epoch [40/100], loss: 1.0308\n",
            "Progress: epoch [50/100], loss: 1.0266\n",
            "Progress: epoch [60/100], loss: 1.0248\n",
            "Progress: epoch [70/100], loss: 1.0242\n",
            "Progress: epoch [80/100], loss: 1.0241\n",
            "Progress: epoch [90/100], loss: 1.0240\n",
            "Progress: epoch [100/100], loss: 1.0240\n",
            "Accuracy of model 4: 50.50%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)  # for reproducibility\n",
        "\n",
        "class SoftmaxRegression(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(SoftmaxRegression, self).__init__()\n",
        "        self.fc = nn.Linear(num_features, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x)\n",
        "        return logits  # no softmax here, since it's included in cross_entropy\n",
        "\n",
        "# Initializing the model\n",
        "num_features = 4\n",
        "num_classes = 3\n",
        "model = SoftmaxRegression(num_features, num_classes)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training \n",
        "for epoch in range(num_epochs):\n",
        "    logits = model(X_train)  \n",
        "    loss = torch.nn.functional.cross_entropy(logits, y_train.long()) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Progress: \" f'epoch [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test) \n",
        "    _, predicted = torch.max(logits, 1)  \n",
        "    correct = (predicted == y_test).sum().item()\n",
        "    accuracy_sm_tnce = 100 * correct / len(y_test)\n",
        "    print(f'Accuracy of model 4: {accuracy_sm_tnce:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5)  Softmax Regression with Mean Squared Error Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: epoch [10/100], loss: 0.2189\n",
            "Progress: epoch [20/100], loss: 0.2111\n",
            "Progress: epoch [30/100], loss: 0.2077\n",
            "Progress: epoch [40/100], loss: 0.2059\n",
            "Progress: epoch [50/100], loss: 0.2051\n",
            "Progress: epoch [60/100], loss: 0.2047\n",
            "Progress: epoch [70/100], loss: 0.2045\n",
            "Progress: epoch [80/100], loss: 0.2045\n",
            "Progress: epoch [90/100], loss: 0.2045\n",
            "Progress: epoch [100/100], loss: 0.2045\n",
            "Accuracy of model 5: 50.07%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)  # for reproducibility\n",
        "\n",
        "class SoftmaxRegression(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(SoftmaxRegression, self).__init__()\n",
        "        self.fc = nn.Linear(num_features, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x)\n",
        "        return F.softmax(logits, dim=1)  # apply softmax \n",
        "\n",
        "# Initialize model\n",
        "num_features = 4\n",
        "num_classes = 3\n",
        "model = SoftmaxRegression(num_features, num_classes)\n",
        "\n",
        "# Initialize training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encoding(labels, num_classes):\n",
        "    labels_long = labels.long().unsqueeze(1)\n",
        "    return torch.zeros(len(labels), num_classes).scatter_(1, labels_long, 1)\n",
        "\n",
        "# MSE Loss function\n",
        "def mse_loss(predictions, one_hot_labels):\n",
        "    return torch.mean((predictions - one_hot_labels) ** 2)\n",
        "\n",
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "    probabilities = model(X_train)\n",
        "    one_hot_labels = one_hot_encoding(y_train, num_classes)\n",
        "    loss = mse_loss(probabilities, one_hot_labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Progress: \" f'epoch [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}')\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    probabilities = model(X_test)\n",
        "    _, predicted = torch.max(probabilities, 1)\n",
        "    correct = (predicted == y_test).sum().item()\n",
        "    accuracy_sm_mse = 100 * correct / len(y_test)\n",
        "    print(f'Accuracy of model 5: {accuracy_sm_mse:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6)  Linear Regression with Mean Squared Error Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: epoch [10/100], loss: 1.8658\n",
            "Progress: epoch [20/100], loss: 1.6058\n",
            "Progress: epoch [30/100], loss: 1.4252\n",
            "Progress: epoch [40/100], loss: 1.2698\n",
            "Progress: epoch [50/100], loss: 1.1360\n",
            "Progress: epoch [60/100], loss: 1.0268\n",
            "Progress: epoch [70/100], loss: 0.9372\n",
            "Progress: epoch [80/100], loss: 0.8646\n",
            "Progress: epoch [90/100], loss: 0.8067\n",
            "Progress: epoch [100/100], loss: 0.7611\n",
            "Accuracy of model 6: 49.50%\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)  # for reproducibility\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.fc = nn.Linear(num_features, 1) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        output = self.fc(x)\n",
        "        return output  # no activation function, direct regression\n",
        "\n",
        "# Initializing the model\n",
        "num_features = 4\n",
        "model = LinearRegression(num_features)\n",
        "\n",
        "# Initializing training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# MSE Loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    continuous_output = model(X_train).squeeze()  \n",
        "    loss = criterion(continuous_output, y_train.float())  \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Progress: \" f'epoch [{epoch+1}/{num_epochs}], loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "with torch.no_grad():\n",
        "    continuous_output = model(X_test).squeeze() \n",
        "    predicted_continuous = continuous_output.round()  \n",
        "    predicted = predicted_continuous.long() \n",
        "    correct = (predicted == y_test).sum().item()\n",
        "    accuracy_lr_mse = 100 * correct / len(y_test)\n",
        "    print(f'Accuracy of model 6: {accuracy_lr_mse:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mModel Description                                            Accuracy       \u001b[0m\n",
            "\u001b[34m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('Logistic Regression for Multiclass Classification', 50.21459227467811),\n",
              " ('Softmax Regression with custom implementation of Cross Entropy Loss',\n",
              "  50.50071530758226)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a function to print in color\n",
        "def print_colored(text, color):\n",
        "    print(colored(text, color))\n",
        "\n",
        "# Header\n",
        "header = \"{:<60} {:<15}\".format(\"Model Description\", \"Accuracy\")\n",
        "print_colored(header, \"blue\")\n",
        "print_colored(\"-\" * 75, \"blue\")\n",
        "\n",
        "# Data\n",
        "data = [\n",
        "    (\"Logistic Regression for Multiclass Classification\", accuracy_ffnn),\n",
        "    (\"Softmax Regression with custom implementation of Cross Entropy Loss\", accuracy_sm_cel)]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model 1: Logistic Regression for Multiclass Classification: 50.21%\n",
            "Accuracy of model 2: Softmax Regression with custom implementation of Cross Entropy Loss: 50.50%\n",
            "Accuracy of model 3: Softmax Regression with torch.nn.functional.nll_loss: 50.50%\n",
            "Accuracy of model 4: Softmax Regression with torch.nn.functional.cross_entropy: 50.50%\n",
            "Accuracy of model 5: Softmax Regression with Mean Squared Error Loss:  50.07%\n",
            "Accuracy of model 6: Linear Regression with Mean Squared Error Loss: 49.50%\n"
          ]
        }
      ],
      "source": [
        "#Comparison of model accuracy\n",
        "print(f'Accuracy of model 1: Logistic Regression for Multiclass Classification: {accuracy_ffnn:.2f}%')\n",
        "print(f'Accuracy of model 2: Softmax Regression with custom implementation of Cross Entropy Loss: {accuracy_sm_cel:.2f}%')\n",
        "print(f'Accuracy of model 3: Softmax Regression with torch.nn.functional.nll_loss: {accuracy_sm_tn:.2f}%')\n",
        "print(f'Accuracy of model 4: Softmax Regression with torch.nn.functional.cross_entropy: {accuracy_sm_tnce:.2f}%')\n",
        "print(f'Accuracy of model 5: Softmax Regression with Mean Squared Error Loss:  {accuracy_sm_mse:.2f}%')\n",
        "print(f'Accuracy of model 6: Linear Regression with Mean Squared Error Loss: {accuracy_lr_mse:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "assignment_1B_2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
